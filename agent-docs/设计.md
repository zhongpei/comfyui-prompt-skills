# **基于OpenCode SDK与GLM-4构建下一代ComfyUI智能提示词生成系统的深度研究报告**

## **1\. 绪论：生成式工作流的代理化转型**

随着生成式人工智能（Generative AI）技术的指数级迭代，图像生成领域的重心已从基础模型的参数开始转向工作流的精准控制与自动化编排。ComfyUI作为基于节点图（Node-based）的生成界面，凭借其极高的权限成为专业创作者的首选工具。然而，ComfyUI的复杂性至于构成了大规模的认知拓扑：用户不仅需要掌握复杂的节点连接逻辑，还需要针对不同的模型架构（如SDXL、Flux、Z-Image）记忆海量的特定提示词（Prompt Engineering）与风格触发词。

在此背景下，将大语言模型（LLM）引入生成工作流，充当“认知中间件”，已成为行业发展的必然趋势。本报告旨在探讨一种基于**OpenCode SDK**构建的代理化（Agentic）提示词生成系统。该系统并不是简单的文本补全工具，而是一个具备自主推理能力的智能体，它通过**ZhipuAI GLM-4**模型认知提供支持，利用OpenCode的事件驱动架构管理上下文，并针对**Z-Image**这一新兴的GPU生成模型进行深度优化的样式库调用。

本研究的核心目标解决了三个关键痛点：

1. **技能的提示封装**：如何将复杂的词工程知识（如漫画创作、写真摄影的特定光影规则）封装为可复用的“技能（Skills）”。  
2. **动态知识搜索**：如何赋予智能体读取并理解外部风格库（如Z-Image内置的数千个风格关键词）的能力，而不依赖于模型幻觉。  
3. **模型架构架构**：针对Z-Image特有的S3-DiT架构与增强特性，制定精准的提示词策略。

## ---

**2\. 技术架构理论框架**

构建一个高可用性的提示词生成插件，首先需要深入了解其基础的运行环境与认知核心。配备OpenCode SDK的代理架构、ZhipuAI GLM-4的认知特性以及Z-Image的生成工具进行剖析。

### **2.1 OpenCode SDK：终端驻留型智能体架构**

OpenCode代表了AI辅助编码工具的一种新范式。与传统的IDE插件（如Copilot）不同，OpenCode采用了一种\*\*客户端服务器（Client-Server）\*\*的分离架构，这使得它能够作为一个独立的运行时环境（Runtime），存在而不仅仅是文本编辑器的一个扩展功能1。

#### **2.1.1 MCP模型上下文协议与工具链**

OpenCode 的核心对抗在于其对\*\*模型上下文协议（Model Context Protocol, MCP）\*\*的实现。MCP 提供了一种标准化的方式，使得 LLM 能够安全、受控地与外部环境进行交互2在提示词生成器的场景中，这意味着我们的智能体不仅仅是在“聊天”，它还可以执行实际的工具调用（Tool Calls）：

* **文件系统操作（fs tools）**：智能体可以读取本地的JSON风格库，搜索特定的艺术家或流派关键词。  
* **Shell 执行（bash 工具）**：智能体可以执行脚本，甚至直接调用 Python 脚本来代替数据。  
* **状态管理**：OpenCode 维护着一个持久化的会话（Session），这使得用户可以在多轮对话中渐进修改提示词，例如“把刚才生成的赛博朋克风格改得更黑暗一点”，而智能体能够准确理解“刚才”的指代对象1。

#### **2.1.2 技能（Skills）的懒加载机制**

根据 OpenCode 的设计文档，\*\*技能（Skills）是定义代理行为的核心单元。一个技能本质上是一个包含了 YAML 元数据（Frontmatter）和 Markdown 指令集的目录3OpenCode采用了一种高效的懒加载（Lazy Loading）**机制。系统不会将所有技能的详细指令一次性塞入LLM的上下文窗口（Context Window），从而造成注意力分散（Attention Dilution）和令牌浪费。相反，系统仅将技能的**提示描述（Description）\*\*注入系统提示词。当用户的意图与某个描述匹配时（例如用户输入“帮我生成一张二次元图”），LLM它会通过工具调用加载该技能的完整SKILL.md文件。机制对于本插件至关重要，因为允许我们内置一套针对Z-Image的复杂风格模板（如“80年代胶片”、“皮克斯3D”、“水墨画”），而不会拖慢常规对话的。

### **2.2 智普AI GLM-4：双语认知引擎**

在选择驱动智能体的核心模型时，**智普AI GLM-4**表现出了针对Z-Image工作流的独特优势。

#### **2.2.1 双语视觉边缘**

Z-Image（Tongyi-MAI研发）是一个在中文数据集上进行混合训练的模型6这意味着该模型对于某些特定的中文文化概念（如“仙侠”、“赛博修真”）已经有了初步级别的理解，而这些概念在纯中文模型（如SDXL卫）中往往能够实现极其复杂的中文描述才能还原。GLM-4作为当前最强的国产大模型之一，需要具备、卓越的中英双语推理能力。它可以理解用户模糊的中文指令（如“那王家电影的感觉”），将其策略翻译为Z-Image能够理解的视觉效果（如“运动模糊、霓虹灯、高对比度、忧郁气氛、王家卫风格”）7。 跨这种语言的语义扫描能力，是很难使用GPT-4或克劳德难以企及的，因为黎明对中文互联网特有的视觉模因理解较浅。

#### **2.2.2 长上下文与复杂推理**

GLM-4 支持 128k 的上下文窗口9虽然我们利用 OpenCode 的 RAG 机制来检索风格，但在某些复杂的场景下（例如用户上传了一段几千字的片段小说，要求其中的场景生成插图），GLM-4 能够一次性吞吐大量文本，提取核心视觉元素，并结合 Z-Image 的技术约束生成 JSON 格式的提示词，锻炼出强大的长文本处理与逻辑推理能力。

### **2.3 Z-Image S3-DiT：生成架构的范式迁移**

要开发高质量的提示词插件，必须理解 Z-Image 的基础架构，因为架构决定了提示词的生效机制。

#### **2.3.1 可扩展单流 DiT 架构 (S3-DiT)**

传统的扩散模型（如Stable Diffusion 1.5）多采用U-Net架构，文本编码器（CLIP）通过交叉注意力层（Cross-Attention）注入信息。而Z-Image采用了**可扩展单流扩散变压器（S3-DiT）**架构6在S3-DiT中，文本令牌和图像潜在令牌被连接成一个单一的序列，输入到Transformer骨干网络中进行处理。这种架构带来的显着影响是：

* **语义保真度极高**：模型对提示词的遵循程度（Prompt Adherence）远超U-Net模型。提示词中的每个词都直接参与了全局自焦点计算，因此不需要像SD1.5那样使用大量的权重(word:1.5)来强调强调重点。  
* **无意见性（Unopinionated）**：Z-Image Turbo 被设计为“无特定风格偏好”10如果用户输入简单的“一个女孩”，模型会生成一个提示平淡、类似3D渲染的通用图像。要获得高质量的图像，必须显式地光影、材质和摄影参数。这大大增加了描述词编写的分量，也是提示生成器存在的意义。

## ---

**3\. Z-Image风格库深度调研与分类学构建**

为了满足需求中“特定读取模型内置风格关键词库”的要求，我们对Z-Image社区的开源资源、案例库及技术文档进行了消费的调研10我们识别了Z-Image响应极其敏感的三大风格集群，并整理了具体的触发关键词。

### **3.1 摄影写实主义（Photorealism）资源**

Z-Image Turbo 在生成照片级图像时，如果不加修饰，往往会产生“塑料感”或“过度平滑”的α影。研究发现，通过物理模拟星巴克的缺陷，可以极大提升真实感13。

| 风格子类（子类别） | 核心触发词（Trigger keywords） | 视觉效果与原理（机制） |
| :---- | :---- | :---- |
| **模拟胶片（模拟胶片）** | shot with point-and-shoot，，，，，disposable camera​film grain​Kodak Portra 400​high ISO​light leak | **强制噪声注入**：通过引入“薄膜颗粒”和“瞬间相机”的描述，打破模型生成的完美平滑表面，增加皮肤纹理的随机性。 |
| **抓拍纪实 (Candid)** | candid photography，，，，，imperfect framing​motion blur​flash photography​red eye​amateur shot | **构图解构**：Z-Image倾向于生成居中构图。这些词汇强制模型采用更自然的、非摆拍的构图方式。 |
| **商业棚拍 (Studio)** | studio lighting，，，，，rim light​85mm lens​sharp focus​volumetric lighting​softbox | **光影亮度**：明确光源光源方向和亮度，消除Z-Image默认的平光（Flat Lighting）依赖性。 |
| **街头摄影（街头）** | street photography，，，，f/8​deep depth of field​Leica M6​urban texture | **景深控制**：控制背景的虚化程度，增强场景的叙述性。 |

### **3.2 二次元与插画（动漫&插画）资源**

Z-Image在动漫风格上表现出色，但很容易受到3D训练数据的污染，导致生成的动漫人物具有不自然的立体感。

| 风格子类（子类别） | 核心触发词（Trigger keywords） | 视觉效果与原理（机制） |
| :---- | :---- | :---- |
| **复古动漫（复古动漫）** | 1990s anime screenshot，，，，cel shaded​low resolution​VHS glitch​hand drawn | **降维打击**：（cel shaded赛璐璐着色）是关键，它强制模型放弃渐变颜色，使用色块渲染，从而获得纯正的2D模型。 |
| **面料（矢量/平面）** | flat color，，，，vector art​thick outline​minimalist​Adobe Illustrator | **线条强化**：thick outline能够有效分离主体与背景，模拟矢量图的视觉特征。 |
| **吉卜力风格 (Ghibli)** | Ghibli style，，，detailed background painting​cumulonimbus clouds​pastoral atmosphere | **场景**：不仅改变画风，还引入了特定的环境元素（如巨大的积雨云、草地），这是Z-Image联想记忆的一部分。 |
| **厚涂风格（Impasto）** | oil painting，，，thick brushstrokes​textured canvas​palette knife | **材质模拟**：引入纹理，使图像剥离数字绘制的光滑感。 |

### **3.3 3D与设计（3D & Design）资源**

这是Z-Image最“优秀”的默认领域，但要生成高审美素材，仍需特定指令。

| 风格子类（子类别） | 核心触发词（Trigger keywords） | 视觉效果与原理（机制） |
| :---- | :---- | :---- |
| **盲盒公仔 (Blind Box)** | pop mart style，，，，，chibi​3d render​PVC texture​glossy finish​studio lighting | **材质指定**：明确PVC或陶瓷材质，配合Q版比例。 |
| **极简图标 (Voxel Icon)** | voxel art，，，，isometric view​c4d render​octane render​clay material | **渲染引擎模拟**：Octane render是一个极强的权重词，它可以让Z-Image模仿渲染引擎的光线追踪效果（Global Illumination）。 |
| **超现实主义（超现实主义）** | double exposure，，，dreamscape​impossible geometry​Dali style | **逻辑重组**：利用double exposure（双重曝光）可以生成复杂的逻辑融合效果12。 |

### **3.4 提示词结构的最佳实践**

调研还发现，Z-Image 对**JSON 格式**或**高度构造**的提示词响应极佳14与自然语言的“意识流”相比，构造输入能更好地被S3-DiT模型的注意力解析机制。

**推荐结构：**

JSON

{  
  "主体 (Subject)": "详细描述，包含衣着、动作",  
  "环境 (Environment)": "地理位置、天气、时间",  
  "媒介 (Medium)": "摄影/插画/3D",  
  "风格修饰 (Style Modifiers)": "上述表格中的关键词",  
  "技术参数 (Tech Specs)": "镜头、灯光、渲染器"  
}

OpenCode插件的生成逻辑应严格遵循此结构，最后将其展平为分隔分隔的字符串以供ComfyUI使用，或直接传递JSON给支持该格式的自定义节点。

## ---

**4.系统架构设计**

基于理论与调研，我们设计了如下的系统架构。该系统由三个主要部分组成：OpenCode 智能体环境、本地风格知识库、以及 ComfyUI 侧的接收节点。

### **4.1 数据流与控制流**

1. **用户输入**：用户在终端通过OpenCode CLI输入模糊指令（如：“生成一组赛博朋克风格的武士，要那种类似《银翼杀手》的电影感”）。  
2. **技能路由（Skill Routing）**：OpenCode智能体分析清晰，识别出关键词“电影感”、“赛博朋克”，决定调用z-skill-cinematic（电影感技能）。  
3. **知识搜索（RAG）**：z-skill-cinematic技能被激活，其指令要求智能体读取本地的z\_styles\_db.json文件，搜索与“赛博朋克”和“银翼杀手”相关的命令行触发词（如、、、neon noir）。anamorphic lensrainy street  
4. **GLM-4推理生成**关键词：智能体将搜索到的与用户的主体描述（“武士”）结合，利用GLM-4的推理能力构造提示词，并翻译为中文。  
5. **工件输出（Artifact Generation）**：智能体将生成的提示词封装为 JSON 文件，写入到与 ComfyUI 共享的目录中。  
6. **ComfyUI 执行**：ComfyUI 中的自定义节点监控该目录，自动加载最新的 JSON 文件并驱动 Z-Image 生成图像。

### **4.2知识库设计：z\_styles\_db.json**

为了满足需求2，我们需要构建一个可扩展的风格数据库。JSON格式是因为它很容易采用被LLM解析并支持结构。

JSON

{  
  "meta": {  
    "model\_version": "Z-Image-Turbo",  
    "last\_updated": "2026-01-07"  
  },  
  "styles": {  
    "photography": \[  
      {  
        "id": "film\_noir",  
        "name": "Film Noir",  
        "keywords": \["black and white", "high contrast", "shadows", "dramatic lighting", "venetian blinds shadows", "1940s style"\],  
        "tech\_specs": "shot on 35mm film, grainy, Kodak Tri-X 400"  
      },  
      {  
        "id": "hasselblad\_studio",  
        "name": "High-End Portrait",  
        "keywords":,  
        "tech\_specs": "85mm f/1.2 lens, softbox lighting, rembrandt lighting"  
      }  
    \],  
    "illustration":,  
        "tech\_specs": "manga page scan, high contrast"  
      }  
    \]  
  }  
}

### **4.3 项目目录结构**

该插件应遵循 OpenCode 的标准项目结构，便于分发和版本控制。

巴什

comfyui-z-prompt-agent/  
├──.opencode/  
│   ├── config.json             \# GLM-4 API 配置  
│   ├── data/  
│   │   └── z\_styles\_db.json    \# 核心风格知识库  
│   ├── skills/  
│   │   ├── z-router/           \# 路由技能  
│   │   │   └── SKILL.md  
│   │   ├── z-photo/            \# 摄影技能  
│   │   │   └── SKILL.md  
│   │   └── z-manga/            \# 漫画技能  
│   │   │   └── SKILL.md  
│   └── scripts/  
│       └── validate\_json.py    \# 辅助脚本，用于校验生成的 JSON 格式  
├── comfy\_nodes/  
│   ├── \_\_init\_\_.py             \# ComfyUI 节点入口  
│   └── z\_bridge\_node.py        \# Python 节点实现  
├── prompts/                    \# 输出目录  
│   └──.keep  
└── AGENTS.md                   \# 全局智能体指令

## ---

**5\. OpenCode 技能开发详解**

首先将展示如何编写核心的SKILL.md文件。这些文件不仅仅是文档，它们是执行的代码（提示代码），直接控制着 GLM-4 的行为逻辑。

### **5.1 技能一：风格路由器（z-router）**

该技能负责“分诊”，防止非专业的技能远程生成过程。

**文件路径**：.opencode/skills/z-router/SKILL.md

## ---

**名称：z-router 描述：当用户请求生成图像、提示词或通过 Z-Image 创作时，使用此技能来分析意图并分发任务。 许可证: MIT**

# **Z-Image风格路由专家**

是一个专门负责分析用户艺术创作的智能体。你的任务不是直接生成提示词，而是用户了解你想要什么风格，并调用相应的子技能。

## **任务流程**

1. **意图分析**：分析用户的输入。  
   * 如果包含“照片”、“真实”、“像真的”、“人像” \-\> 路由至z-photo技巧。  
   * 如果包含“漫画”、“二次元”、“动漫”、“插画”-\>路由至z-manga技能。  
   * 如果包含“图标”、“3D”、“渲染”、“设计”-\>路由至z-design技能。  
2. **歧义处理**：  
   * 如果用户只指定“画一个女孩”，没有指定风格，您需要追问：“请问您想要是写实照片风格，还是二次元动漫风格？”  
3. **执行规则**：  
   * 不要自己尝试生成提示词。必须调用专用技能，因为它们拥有读取风格数据库的权限。

### **5.2 技能二：摄影写实专家（z-photo）**

此技能展示了如何利用OpenCode的工具链读取外部文件（需求点2的核心实现）。

**文件路径**：.opencode/skills/z-photo/SKILL.md

## ---

**名称：z-photo 描述：专门用于生成Z-Image模型的写实摄影类提示词。具备读取风格库的能力。**

# **Z-Image摄影提示词工程师**

你了解摄影光学原理和 Z-Image Turbo 模型的特性。你的目标是编写能欺骗人眼睛的写实提示词。

## **核心能力：风格搜索 (RAG)**

在开始生成之前，您必须执行以下操作：

1. **读取知识库**：使用fs\_read工具读取.opencode/data/z\_styles\_db.json文件。  
2. **风格匹配**：在 JSON 的photography类别中，寻找与用户描述最匹配的风格对象。  
   * 例如：用户说“复古感”，你应该提取film\_noir或analog\_trash的keywords和tech\_specs。

## **提示词构建法则（Z 公式）**

Z-Image 是单一流模型，请按照以下顺序构建提示词（必须翻译为中文）：

\[风格触发词\], \[技术参数\], \[主体描述\], \[环境与光影\], \[构图\]

* **风格触发词**：来自 JSON 数据库（如“shot on 35mm film”）。  
* **主题描述**：极其详细。不要说“一个女人”，而是说“一个25岁的法国女人，雀斑，凌乱的发髻，穿着红色丝质上衣”。  
* **光影**：光源指定（如“自然窗光”）。

## **输出规范**

将结果生成为一个JSON文件，保存到prompts/目录下。文件名格式为photo\_\[timestamp\].json。

JSON 内容示例：json { "positive\_prompt": "使用 35mm 胶片拍摄，柯达 Portra 400，颗粒感强，人物特写，自然光", "negative\_prompt": "", "seed": 123456 }

\*\*注意\*\*：如果是 Z-Image Turbo 模型，\*\*negative\_prompt 必须留空\*\*，因为蒸馏模型不仅忽略负面提示词，有时反而会引入伪影 \[10, 15\]。

### **5.3技能三：二次元创作专家（z-manga）**

针对创作的特定需求。

**文件路径**：.opencode/skills/z-manga/SKILL.md

## ---

**名称：z-manga 描述：专门用于生成Z-Image模型的、漫画和插画风格提示词。**

# **Z-Image二次元提示词工程师**

你专注于2D艺术创作。Z-Image倾向于生成3D效果，你的任务是通过提示词强化其“摩擦化”。

## **风格搜索**

1. **读取知识库**：使用fs\_read工具读取.opencode/data/z\_styles\_db.json。  
2. **搜索类别**：关注illustration和anime备份。

## **创作原则**

1. **反3D策略**：必须包含数据库中的flat color, vector,cel shaded等词汇。  
2. **构图强化**：动漫风格依赖强烈的透视。适当添加fisheye lens（鱼眼）或from below（仰视）来增加张力。  
3. **文化阅读**：如果用户特定提到的中国风（如“国漫”），请使用wuxia style（ink wash painting水墨）等词汇，Z-Image对此有很好的支持。

## **输出操作**

生成JSON文件至prompts/目录。

## ---

**6\. GLM-4 API 集成与配置**

OpenCode默认支持OpenAI协议，这使得接入智普AI变得非常直接，但需要注意API版本的兼容性。

### **6.1 配置详细文件解**

我们需要在项目根目录或用户全局配置中编辑opencode.json。

JSON

{  
  "$schema": "https://opencode.ai/config.json",  
  "provider": {  
    "zhipu": {  
      "npm": "@ai-sdk/openai-compatible",  
      "options": {  
        "baseURL": "https://open.bigmodel.cn/api/paas/v4",   
        "name": "ZhipuAI",  
        "headers": {  
            "Authorization": "Bearer YOUR\_API\_KEY"  
        }  
      }  
    }  
  },  
  "models": {  
    "glm-4-plus": {  
      "provider": "zhipu",  
      "model": "glm-4-plus",  
      "description": "ZhipuAI旗舰模型，用于复杂的提示词推理和风格检索"  
    },  
    "glm-4-flash": {  
      "provider": "zhipu",  
      "model": "glm-4-flash",  
      "description": "高速低成本模型，用于简单的分类或路由任务"  
    }  
  }  
}

**关键配置说明** 8：

* **baseURL** : 必须精确为https://open.bigmodel.cn/api/paas/v4。这是智谱 AI 提供的兼容 OpenAI SDK 的入口。  
* **glm-4-plus** : 推荐用于z-photo和z-manga技能，因为它的指令遵循能力（指令跟随）更强，能更好地处理 JSON 格式约束。  
* **glm-4-flash** : 推荐用于z-router，因为路由任务简单，且 Flash 模型响应极快，可以减少用户等待时间。

### **6.2 环境指标管理**

为了安全起见，不宜将 API Key 硬编码在 json 文件中。建议使用 OpenCode 的auth命令：

巴什

opencode auth login  
\# 选择 "Custom/Other"  
\# 输入 Provider ID: zhipu  
\# 输入 Key: \<你的智谱API Key\>

## ---

**7\. ComfyUI 节点开发 (Python)**

为了实现从OpenCode到ComfyUI的“最后一公里”传输，需要我们开发一个自定义节点。该节点不是简单地动态读取文件，而应具备刷新和格式解析能力。

### **7.1 节点设计思路**

在 ComfyUI 中，INPUT\_TYPES方法决定了节点的 UI 表现。我们需要一个下拉菜单（Combo Box），能够实时显示 OpenCode 生成的 JSON 文件列表17。

由于 ComfyUI 的标准机制是在启动时加载节点定义，动态更新列表通常需要刷新页面。但我们可以通过 Python 的文件扫描机制在每次点击节点时尝试获取最新列表（或者至少在重启工作流时更新）。

### **7.2 代码实现：ZPromptLoader**

创建文件comfy\_nodes/z\_bridge\_node.py：

Python

import os  
import json  
import folder\_paths  
from server import PromptServer

class ZPromptLoader:  
    """  
    OpenCode Z-Image Bridge Node  
    Reads JSON prompt files generated by the OpenCode Agent.  
    """  
    def \_\_init\_\_(self):  
        pass  
      
    @classmethod  
    def INPUT\_TYPES(s):  
        \# 设定提示词文件的扫描路径  
        \# 假设 prompts 文件夹位于 ComfyUI 根目录下的 'opencode\_prompts' 中  
        \# 实际部署时可根据需要调整路径逻辑  
        base\_dir \= folder\_paths.base\_path  
        prompt\_dir \= os.path.join(base\_dir, "opencode\_prompts")  
          
        if not os.path.exists(prompt\_dir):  
            os.makedirs(prompt\_dir)  
              
        \# 扫描所有.json 文件  
        files \= \[f for f in os.listdir(prompt\_dir) if f.endswith(".json")\]  
        \# 按修改时间倒序排列，确保最新的提示词在最上面  
        files.sort(key=lambda x: os.path.getmtime(os.path.join(prompt\_dir, x)), reverse=True)  
          
        if not files:  
            files \= \["no\_prompts\_found.json"\]

        return {  
            "required": {  
                "prompt\_file": (files, {"default": files}),  
            }  
        }

    RETURN\_TYPES \= ("STRING", "STRING", "INT")  
    RETURN\_NAMES \= ("positive", "negative", "seed")  
    FUNCTION \= "load\_prompt"  
    CATEGORY \= "OpenCode/Z-Image"

    def load\_prompt(self, prompt\_file):  
        base\_dir \= folder\_paths.base\_path  
        file\_path \= os.path.join(base\_dir, "opencode\_prompts", prompt\_file)  
          
        if not os.path.exists(file\_path):  
            return ("", "", 0)  
          
        try:  
            with open(file\_path, 'r', encoding='utf-8') as f:  
                data \= json.load(f)  
        except Exception as e:  
            print(f"\[OpenCode\] Error loading JSON: {e}")  
            return ("", "", 0)  
              
        \# 解析标准字段  
        positive \= data.get("positive\_prompt", "")  
        negative \= data.get("negative\_prompt", "")  
        seed \= data.get("seed", 0)  
          
        \# Z-Image Turbo 特殊处理：如果检测到 Turbo 模式，强制清空负面提示词  
        \# 这一步也可以在 Python 端做双重保险  
        if "turbo" in positive.lower() or "z-image-turbo" in str(data.get("model", "")).lower():  
            negative \= ""  
              
        return (positive, negative, seed)

\# 节点注册映射  
NODE\_CLASS\_MAPPINGS \= {  
    "ZPromptLoader": ZPromptLoader  
}

NODE\_DISPLAY\_NAME\_MAPPINGS \= {  
    "ZPromptLoader": "OpenCode Z-Image Bridge"  
}

### **7.3\_\_init\_\_.py入口文件**

Python

from.z\_bridge\_node import NODE\_CLASS\_MAPPINGS, NODE\_DISPLAY\_NAME\_MAPPINGS

\_\_all\_\_ \=

### **7.4 安装与部署**

1. 将comfy\_nodes文件夹内的内容复制到ComfyUI/custom\_nodes/ComfyUI-OpenCode-Bridge/。  
2. 在 ComfyUI 根目录下创建opencode\_prompts文件夹。  
3. 配置 OpenCode 插件，生成生成的 JSON 文件直接写入path/to/ComfyUI/opencode\_prompts/。

## ---

**8.实际操作工作流演示**

另外将通过一个完整的产品案例，展示系统如何良好的工作。

### **8.1 场景：生成一张“90年代香港街头的赛博朋克电影海报”**

第一步：OpenCode交互  
用户在终端运行opencode，进入交互界面。  
用户：“我制作了一张海报，风格类似90年代的香港电影，但是请赛博朋克元素，主角是一个穿着雨衣的侦探。”  
**第二步：智能体推理（GLM-4 \+ 技能）**

1. **关键词路由**：z-router技能识别出“电影”、“海报”，激活z-photo技能（假设我们将电影感等同于广义摄影/写实）。  
2. **数据检索**：z-photo技能读取z\_styles\_db.json。  
3. **风格匹配**：  
   * 智能体搜索JSON，找到了wong\_kar\_wai(王家卫风格)的关键词：motion blur, neon lights, high contrast, melancholy。  
   * 同时检索cyberpunk关键词：holograms, wet street, futuristic。  
4. **提示词构建**：GLM-4 将这些芯片融合，并翻译用户描述。  
   * *提示*：“电影海报风格，35毫米胶片拍摄，王家卫风格，动态模糊，霓虹灯，夜晚的香港街道，大雨，身穿透明雨衣的侦探，背景中的赛博朋克全息影像，忧郁的氛围，颗粒感。”  
5. **文件生成**：OpenCode 读取opencode\_prompts/poster\_hk\_cyber.json。

**第三步：ComfyUI 生成**

1. 用户在ComfyUI界面点击“刷新”。  
2. 在OpenCode Z-Image Bridge节点中选择poster\_hk\_cyber.json。  
3. 节点解析出 Positive Prompt 和 Seed。  
4. 连接至Z-Image Turbo Loader和KSampler。  
5. 点击队列提示。  
6. **结果**：Z-Image Turbo 在约 1 秒内生成了一张具有强烈色彩对比、胶片颗粒感和特定情感动力的高质量图像，完美洞察了默认的“塑料 3D”曲面。

## ---

**9.结论与展望**

本报告详细阐述了如何利用 OpenCode SDK 和 GLM-4 构建一个专业的 ComfyUI 提示词生成系统。通过将提示词工程解构为**即时的技能（Skills）和格式化的数据（Style DB）**，我们成功解决了记忆负担过重和风格一致性难以维持的问题。

本系统的核心创新点在于：

1. **代理式 RAG**：自动体自主读取本地风格库，实现了知识与模型的解耦合。风格库可以由社区无限扩展，而重新占用模型。  
2. **双语优势的最大化**：利用GLM-4的中英文能力，弥补了Z-Image在纯英文提示词环境下的文化隔阂。  
3. **架构架构性**：针对Z-Image S3-DiT架构定制的提示词策略（如强制摄影参数、废除负面提示词），显着提升了图像生成的审美上限。

未来，该系统可进一步扩展支持支持**数据库（Vector DB）**以处理数以万计的风格词条，或集成**GLM-4V**视觉模型，实现“看图写词”的逆向工程能力，从而形成一个闭环的AI创作生态系统。

#### **Works cited**

1. SDK \- OpenCode, accessed January 7, 2026, [https://opencode.ai/docs/sdk/](https://opencode.ai/docs/sdk/)  
2. opencode-ai/opencode: A powerful AI coding agent. Built for the terminal. \- GitHub, accessed January 7, 2026, [https://github.com/opencode-ai/opencode](https://github.com/opencode-ai/opencode)  
3. Agent Skills | OpenCode, accessed January 7, 2026, [https://opencode.ai/docs/skills/](https://opencode.ai/docs/skills/)  
4. malhashemi/opencode-skills \- GitHub, accessed January 7, 2026, [https://github.com/malhashemi/opencode-skills](https://github.com/malhashemi/opencode-skills)  
5. How to use the skills in v1.0.186 : r/opencodeCLI \- Reddit, accessed January 7, 2026, [https://www.reddit.com/r/opencodeCLI/comments/1pt8oeu/how\_to\_use\_the\_skills\_in\_v10186/](https://www.reddit.com/r/opencodeCLI/comments/1pt8oeu/how_to_use_the_skills_in_v10186/)  
6. Z-Image ComfyUI Workflow Example, accessed January 7, 2026, [https://docs.comfy.org/tutorials/image/z-image/z-image-turbo](https://docs.comfy.org/tutorials/image/z-image/z-image-turbo)  
7. OpenCode \- Overview \- Z.AI DEVELOPER DOCUMENT, accessed January 7, 2026, [https://docs.z.ai/scenario-example/develop-tools/opencode](https://docs.z.ai/scenario-example/develop-tools/opencode)  
8. SDK Code Examples \- ZHIPU AI OPEN PLATFORM, accessed January 7, 2026, [https://open.bigmodel.cn/dev/api/devguide/sdk\_example](https://open.bigmodel.cn/dev/api/devguide/sdk_example)  
9. Z.ai API, accessed January 7, 2026, [https://docs.z.ai/guides/overview/quick-start](https://docs.z.ai/guides/overview/quick-start)  
10. Z-Image styles: 70 examples of how much can be done with just prompting. \- Reddit, accessed January 7, 2026, [https://www.reddit.com/r/StableDiffusion/comments/1pdy78q/zimage\_styles\_70\_examples\_of\_how\_much\_can\_be\_done/](https://www.reddit.com/r/StableDiffusion/comments/1pdy78q/zimage_styles_70_examples_of_how_much_can_be_done/)  
11. Z-Image Turbo Prompt Guide: Master AI Image Generation in 2025 | fal, accessed January 7, 2026, [https://fal.ai/learn/devs/z-image-turbo-prompt-guide](https://fal.ai/learn/devs/z-image-turbo-prompt-guide)  
12. camenduru/awesome-z-image-turbo \- GitHub, accessed January 7, 2026, [https://github.com/camenduru/awesome-z-image-turbo](https://github.com/camenduru/awesome-z-image-turbo)  
13. The Secrets of Realism, Consistency and Variety with Z Image Turbo : r/StableDiffusion, accessed January 7, 2026, [https://www.reddit.com/r/StableDiffusion/comments/1pcxtba/the\_secrets\_of\_realism\_consistency\_and\_variety/](https://www.reddit.com/r/StableDiffusion/comments/1pcxtba/the_secrets_of_realism_consistency_and_variety/)  
14. JSON prompts better for z-image? : r/StableDiffusion \- Reddit, accessed January 7, 2026, [https://www.reddit.com/r/StableDiffusion/comments/1peuwt7/json\_prompts\_better\_for\_zimage/](https://www.reddit.com/r/StableDiffusion/comments/1peuwt7/json_prompts_better_for_zimage/)  
15. In latest version v0.11.3 How to config and use GLM-4.5 model if there isn't in the list? · Issue \#2760 · sst/opencode \- GitHub, accessed January 7, 2026, [https://github.com/sst/opencode/issues/2760](https://github.com/sst/opencode/issues/2760)  
16. ComfyUI\_unified\_list\_selector Custom Node \- ComfyAI.run, accessed January 7, 2026, [https://comfyai.run/custom\_node/ComfyUI\_unified\_list\_selector](https://comfyai.run/custom_node/ComfyUI_unified_list_selector)  
17. Datatypes \- ComfyUI Official Documentation, accessed January 7, 2026, [https://docs.comfy.org/custom-nodes/backend/datatypes](https://docs.comfy.org/custom-nodes/backend/datatypes)